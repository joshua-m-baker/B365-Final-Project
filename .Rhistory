h.cut
h$v35
h.new = cbind(h.cut, d2.data$V35)
h.new
h.comparison = cbind(h.cut, d2.data$V35)
g1 = h.comparison[h.comparison == 1]
g1
g1 = h.comparison[h.comparison == 1,]
g1 = sum(h.comparison == 1)
sum(h.comparison == 1)
sum(h.comparison == "g")
sum(h.comparison == "b")
h.comparison
sum(h.comparison == 2)
sum(h.comparison == "g")
sum(h.comparison == "b") #
x = h.comparison == 1
x
x = h.comparison == 1, "g"
x = h.comparison[h.comparison[,1] == 1 & h.comparison[,2] "g"]
x = h.comparison[h.comparison[,1] == 1 & h.comparison[,2] == "g"]
x
length(x)
sum(x==1)
x = h.comparison[h.comparison[,1] == 1 & h.comparison[,2] == "b"]
sum(x == 1) #34
x = h.comparison[h.comparison[,1] == 2 & h.comparison[,2] == "g"]
sum(x == 1) #9
sum(x == 1) #0
x
sum(x == 2) #0
h.pca = prcomp(d2.data)
head(d2.data)
h.pca = prcomp(d2.data[,-35])
h.pca
summary(h.pca)
.pca = prcomp(d2.data[,-35])
d.pca = prcomp(d2.data[,-35])
summary(d.pca)
d.pca = prcomp(d2.data[,-35])
summary(d.pca)
d.reduced = d.pca$x[,1:10]
d.reduced
h.reduced = hclust(dist(d.reduced))
plot(h.reduced)
plot(h)
plot(h.reduced)
h2 = hclust(dist(d.reduced))
plot(h2)
h2.cut = cutree(h2, k=2)
h.comparison = cbind(h.cut, d2.data$V35)
h2.comparison = cbind(h.cut, d2.data$V35)
h2.comparison
h2.comparison = cbind(h2.cut, d2.data$V35)
h2.comparison
d.pca = prcomp(d2.data[,-35])
summary(d.pca)
d.reduced = d.pca$x[,1:10]
h2 = hclust(dist(d.reduced))
plot(h2)
h2.cut = cutree(h2, k=2)
h2.comparison = cbind(h2.cut, d2.data$V35)
h2.comparison
x = h2.comparison[h2.comparison[,1] == 1 & h2.comparison[,2] == "b"]
sum(x == 1) #9
x
h2.comparison
sum(h2.comparison == 1) #Cluster 1 size =
sum(h2.comparison == 2) #Cluster 2 size =
x = h2.comparison[h2.comparison[,1] == 1 & h2.comparison[,2] == "b"]
sum(x == 1) #9
8/39
x = h.comparison[h2.comparison[,1] == 2 & h2.comparison[,2] == "g"]
sum(x == 2) #3
8/39
3/11
x = h2.comparison[h2.comparison[,1] == 1 & h2.comparison[,2] == "b"]
sum(x == 1) #8
8/39
x = h2.comparison[h2.comparison[,1] == 2 & h2.comparison[,2] == "g"]
sum(x == 2) #3
3/11
sum(x == 2) #6
6/11
h2.comparison
sum(h2.comparison == 1) #Cluster 1 size = 39
sum(h2.comparison == 2) #Cluster 2 size = 11
x = h2.comparison[h2.comparison[,1] == 1 & h2.comparison[,2] == "b"]
sum(x == 1) #8
8/39
x = h2.comparison[h2.comparison[,1] == 2 & h2.comparison[,2] == "g"]
sum(x == 2) #6
x = h2.comparison[h2.comparison[,1] == 2 & h2.comparison[,2] == "b"]
sum(x == 2) #6
x = h2.comparison[h2.comparison[,1] == 1 & h2.comparison[,2] == "g"]
sum(x == 1) #8
31/39
d.pca = prcomp(d2.data[,-35])
summary(d.pca)
d.pca$x
d.reduced = d.pca$x[,1:10]
d.reduced
h2 = hclust(dist(d.reduced))
plot(h2)
h2.comparison = cbind(h2.cut, d2.data$V35)
h2.comparison
sum(h2.comparison == 1) #Cluster 1 size = 39
sum(h2.comparison == 2) #Cluster 2 size = 11
x = h2.comparison[h2.comparison[,1] == 1 & h2.comparison[,2] == "g"]
sum(x == 1) #8
x = h2.comparison[h2.comparison[,1] == 1 & h2.comparison[,2] == "b"]
sum(x == 1) #8
8/39
x = h2.comparison[h2.comparison[,1] == 2 & h2.comparison[,2] == "g"]
sum(x == 2) #6
6/11
d.pca = prcomp(d2.data[,-35])
summary(d.pca)
library(data.table)
d2 = fread("https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data")
d2.data = d2[sample(nrow(d2), 50),]
d.pca = prcomp(d2.data[,-35])
summary(d.pca)
d.pca$x
d.reduced = d.pca$x[,1:10]
h2 = hclust(dist(d.reduced))
plot(h2)
h2.cut = cutree(h2, k=2)
h2.comparison = cbind(h2.cut, d2.data$V35)
h2.comparison
sum(h2.comparison == 1) #Cluster 1 size = 39
sum(h2.comparison == 2) #Cluster 2 size = 11
x = h2.comparison[h2.comparison[,1] == 1 & h2.comparison[,2] == "b"]
sum(x == 1) #8
8/39
x = h2.comparison[h2.comparison[,1] == 2 & h2.comparison[,2] == "g"]
sum(x == 2) #6
sum(h2.comparison == 1) #Cluster 1 size = 39
sum(h2.comparison == 2) #Cluster 2 size = 11
.205+.545
.75/2
h2 = hclust(dist(d.reduced), method = "complete")
d2 = fread("https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data")
d2.data = d2[sample(nrow(d2), 50),]
h = hclust(dist(d2.data[,-35]), method = "complete")
plot(h)
h.cut = cutree(h, k=2)
plot(h.cut)
h.comparison = cbind(h.cut, d2.data$V35)
h.comparison
sum(h.comparison == 1) #Cluster 1 size = 43
sum(h.comparison == 2) #Cluster 2 size = 7
h = hclust(dist(d2.data[,-35], method = "euclidean"), method = "complete")
d2 = fread("https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data")
d2.data = d2[sample(nrow(d2), 50),]
h = hclust(dist(d2.data[,-35], method = "euclidean"), method = "complete")
plot(h)
h.cut = cutree(h, k=2)
plot(h.cut)
h.comparison = cbind(h.cut, d2.data$V35)
h.comparison
sum(h.comparison == 1) #Cluster 1 size = 43
sum(h.comparison == 2) #Cluster 2 size = 7
sum(h.comparison == "g") #37
sum(h.comparison == "b") #13
sum(h.comparison == "b") #11
x = h.comparison[h.comparison[,1] == 1 & h.comparison[,2] == "b"]
sum(x == 1) #9
x = h.comparison[h.comparison[,1] == 1 & h.comparison[,2] == "g"]
sum(x == 1) #8
8/20
x = h.comparison[h.comparison[,1] == 2 & h.comparison[,2] == "g"]
sum(x == 2) #3
x = h.comparison[h.comparison[,1] == 2 & h.comparison[,2] == "b"]
sum(x == 2) #g = 27
3/30
8/20
h2 = hclust(dist(d.reduced, method = "euclidean"), method = "complete")
h.comparison
plot(h)
plot(h, main = "2.2.1")
sum(h.comparison == 1) #Cluster 1 size = 20
sum(h.comparison == 2) #Cluster 2 size = 30
sum(h.comparison == "g") #39
sum(h.comparison == "b") #11
x = h.comparison[h.comparison[,1] == 1 & h.comparison[,2] == "g"]
sum(x == 1) #b = 8, g = 12
d.pca = prcomp(d2.data[,-35])
summary(d.pca)
summary(d.pca)
d.reduced = d.pca$x[,1:8]
h2 = hclust(dist(d.reduced, method = "euclidean"), method = "complete")
plot(h2)
plot(h2, main = "2.2.3")
h2.cut = cutree(h2, k=2)
h2.comparison = cbind(h2.cut, d2.data$V35)
h2.comparison
sum(h2.comparison == 1) #Cluster 1 size = 39
sum(h2.comparison == 2) #Cluster 2 size = 11
sum(h.comparison == "g") #39
sum(h.comparison == "b") #11
sum(h2.comparison == "g") #39
sum(h2.comparison == "b") #11
x = h2.comparison[h2.comparison[,1] == 1 & h2.comparison[,2] == "b"]
sum(x == 1) #8
x = h2.comparison[h2.comparison[,1] == 2 & h2.comparison[,2] == "g"]
sum(x == 2) #6
x = h2.comparison[h2.comparison[,1] == 2 & h2.comparison[,2] == "b"]
sum(x == 2) #6
5/36
sum(h2.comparison == 1) #Cluster 1 size = 14
h2.comparison == 1
sum(h2.comparison == 1) #Cluster 1 size = 14
sum(h2.comparison == 2) #Cluster 2 size = 36
x = h2.comparison[h2.comparison[,1] == 1 & h2.comparison[,2] == "b"]
x = h2.comparison[h2.comparison[,1] == 1 & h2.comparison[,2] == "g"]
sum(x == 1) #6
6/14
.429+.139
.568/2
d = fread("https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data")
d.data = d[,-35]
pca = prcomp(d.data)
summary(pca)
plot(pca, type = "l")
plot(pca, type = "l", main = "PC Variance")
plot(h, main = "2.2.1")
plot(h2, main = "2.2.3")
library(data.table)
d = fread("https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data")
d.data = d[,-35]
pca = prcomp(d.data)
summary(pca)
plot(pca, type = "l", main = "PC Variance")
screeplot(pca)
#newdata
pca$x
summary(pca)
pca.reduced = pca$x[,1:18]
pca.reduced
min(1,2)
x = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "g"]
y = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "b"]
x
y
h2.comparison
z = min(sum(x == k), sum(y == k))
err = z/sum(h2.comparison == k)
er
err
z
x = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "g"]
x
h2.comparison
x = h2.comparison[h2.comparison[,1] == 1 & h2.comparison[,2] == "g"]
sum(x == 1) #6
k=1
x = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "g"]
y = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "b"]
z = min(sum(x == k), sum(y == k))
err = z/sum(h2.comparison == k)
err
k=2
x = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "g"]
y = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "b"]
z = min(sum(x == k), sum(y == k))
err = z/sum(h2.comparison == k)
err
kmRes = kmeans(pca.reduced, dist.euclidean, centers = k)
pca.reduced
kmRes = kmeans(pca.reduced, dist.euclidean, centers = k)
kmRes = kmeans(pca.reduced, centers = k)
kmRes
kmRes$cluster
pca.reduced = pca$x[,1:18]
error <- function(data, k){
errors = c(rep(0,k))
for(i in 1:k){
x = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "g"]
y = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "b"]
z = min(sum(x == k), sum(y == k))
errors[i] = z/sum(h2.comparison == k)
}
return(mean(errors))
}
errors = as.data.frame(matrix(0,nrow = 20, ncol = 5))
for(k in 2:5){
errorList = list()
for(j in 1:20){
print(j)
kmRes = kmeans(pca.reduced, centers = k)
newdata = cbind(kmRes$cluster, mydata$V35)
errors[j, k] = error(newdata,k)
}
}
errors
error <- function(data, k){
errorList = c(rep(0,k))
for(i in 1:k){
x = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "g"]
y = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "b"]
z = min(sum(x == k), sum(y == k))
err = z/sum(h2.comparison == k)
print(err)
errorList[i] = err
}
return(mean(errorList))
}
errors = as.data.frame(matrix(0,nrow = 20, ncol = 5))
for(k in 2:5){
errorList = list()
for(j in 1:20){
print(j)
kmRes = kmeans(pca.reduced, centers = k)
newdata = cbind(kmRes$cluster, mydata$V35)
errors[j, k] = error(newdata,k)
}
}
error <- function(data, k){
errorList = c(rep(0,k))
for(i in 1:k){
x = data[data[,1] == k & data[,2] == "g"]
y = data[data[,1] == k & data[,2] == "b"]
z = min(sum(x == k), sum(y == k))
err = z/sum(data == k)
print(err)
errorList[i] = err
}
return(mean(errorList))
}
errors = as.data.frame(matrix(0,nrow = 20, ncol = 5))
for(k in 2:5){
errorList = list()
for(j in 1:20){
print(j)
kmRes = kmeans(pca.reduced, centers = k)
newdata = cbind(kmRes$cluster, mydata$V35)
errors[j, k] = error(newdata,k)
}
}
print(data)
error <- function(data, k){
print(data)
errorList = c(rep(0,k))
for(i in 1:k){
x = data[data[,1] == k & data[,2] == "g"]
y = data[data[,1] == k & data[,2] == "b"]
z = min(sum(x == k), sum(y == k))
err = z/sum(data == k)
errorList[i] = err
}
return(mean(errorList))
}
errors = as.data.frame(matrix(0,nrow = 20, ncol = 5))
for(k in 2:5){
errorList = list()
for(j in 1:20){
print(j)
kmRes = kmeans(pca.reduced, centers = k)
newdata = cbind(kmRes$cluster, mydata$V35)
errors[j, k] = error(newdata,k)
}
}
error <- function(data, k){
errorList = c(rep(0,k))
for(i in 1:k){
x = data[data[,1] == i & data[,2] == "g"]
y = data[data[,1] == i & data[,2] == "b"]
z = min(sum(x == i), sum(y == i))
err = z/sum(data == i)
errorList[i] = err
}
return(mean(errorList))
}
errors = as.data.frame(matrix(0,nrow = 20, ncol = 5))
for(k in 2:5){
errorList = list()
for(j in 1:20){
print(j)
kmRes = kmeans(pca.reduced, centers = k)
newdata = cbind(kmRes$cluster, mydata$V35)
errors[j, k] = error(newdata,k)
}
}
errors
boxplot(errors)
boxplot(errors, main = "PCA Reduced K-means errors")
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
summary(college)
library(test2.R)
import(test2)
library(test2)
college <- scan(file.choose())
p = (m-u)/(s/sqrt(n))
#B)
#H0 = u <= 57,000     H1 = u > 57,000
#reject if H1 << 57,000
#alpha = .05
u=57000
n=340
m = mean(college)
l = length(college)
v = var(college)
s = sd(college)
p = (m-u)/(s/sqrt(n))
t = t.test(college, mu = 57000, alternative = 'l')
t
?t.test
values(t.test)
t$p.value
t = t.test(college, mu = 57000, alternative = 'l')
t = t.test(college, mu = 57000, alternative = 'g')
t
#95 percent confidence interval:
#  36.06240 48.19622
t.test(ozone, mu=50, alternative = 'l')
ozone <- scan(file.choose())
#95 percent confidence interval:
#  36.06240 48.19622
t.test(ozone, mu=50, alternative = 'l')
t.test(ozone)
#95 percent confidence interval:
#  36.06240 48.19622
t.test(ozone, mu=50, alternative = 'l')
t.test(ozone)
mean(ozone)
u = 50
n = length(ozone)
m = mean(ozone)
s = sd(ozone)
z = (m-u)/(s/sqrt(n))
z
1-pnorm(z)
pnorm(z)
qqnorm(college, main = "College qq-plot")
s = sd(college)
n=length(college)
#a =.05, so reject. U > 57,000
z=1.96
i1 = z*(s/sqrt(n))
i1
i1 = z*(s/sqrt(n))
mean + i1
m + i1
m - i1
mean(college)
m = mean(college)
m
i1
m + i1
m - i1
t.test(college)
i1 = z*(s/sqrt(n))
m + i1
m - i1
t = t.test(college)
values(t)
?t.test
t.conf.int
t = t.test(college)
t$conf.int
n = length(ozone)
m = mean(ozone)
s = sd(ozone)
z=1.96
i2 = z*(s/sqrt(n))
m + i2
m - i2
i2
data.train <- read.csv("train.csv", header = TRUE,na.strings = "-1" )
data.test <- read.csv("test.csv", header = TRUE,na.strings = "-1")
#install.packages('mice')
#library('mice')
setwd('C:\\Users\\Joshua\\Documents\\GitHub\\B365-Final-Project')
data.train <- read.csv("train.csv", header = TRUE,na.strings = "-1" )
data.test <- read.csv("test.csv", header = TRUE,na.strings = "-1")
data.train[data.train<0] = NA
data.test[data.test<0] = NA
data.test = cbind(data.test,NA_count = rowSums(is.na(data.test)))
data.train = cbind(data.train,NA_count = rowSums(is.na(data.train)))
test_ids = data.test$id
data.test <- subset(data.test,select = -c(id))
data.train <- subset(data.train,select = -c(id))
training_classes = data.train$target
data.train <- subset(data.train,select = -c(target))
training_size = nrow(data.train)
data.test <- subset(data.test,select = -c(ps_car_03_cat,ps_car_05_cat,ps_reg_03))
data.train <- subset(data.train,select = -c(ps_car_03_cat,ps_car_05_cat,ps_reg_03))
training = na.omit(data.train)
test = na.omit(data.test)
data.train <- read.csv("train.csv", header = TRUE,na.strings = "-1" )
data.test <- read.csv("test.csv", header = TRUE,na.strings = "-1")
data.train[data.train<0] = NA
data.test[data.test<0] = NA
data.test = cbind(data.test,NA_count = rowSums(is.na(data.test)))
data.train = cbind(data.train,NA_count = rowSums(is.na(data.train)))
test_ids = data.test$id
data.test <- subset(data.test,select = -c(id))
data.train <- subset(data.train,select = -c(id))
data.train = na.omit(data.train)
data.test = na.omit(data.test)
training_classes = data.train$target
data.train <- subset(data.train,select = -c(target))
training_size = nrow(data.train)
data.test <- subset(data.test,select = -c(ps_car_03_cat,ps_car_05_cat,ps_reg_03))
data.train <- subset(data.train,select = -c(ps_car_03_cat,ps_car_05_cat,ps_reg_03))
training = data.train
test = data.test
bstDense <- xgboost(data = as.matrix(training), label = training_classes, missing = NA, max_depth = 10, eta = 1, nthread = 2, nrounds = 5, objective = "binary:logistic")
library('xgboost')
bstDense <- xgboost(data = as.matrix(training), label = training_classes, missing = NA, max_depth = 10, eta = 1, nthread = 2, nrounds = 5, objective = "binary:logistic")
