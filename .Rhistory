h2.comparison
z = min(sum(x == k), sum(y == k))
err = z/sum(h2.comparison == k)
er
err
z
x = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "g"]
x
h2.comparison
x = h2.comparison[h2.comparison[,1] == 1 & h2.comparison[,2] == "g"]
sum(x == 1) #6
k=1
x = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "g"]
y = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "b"]
z = min(sum(x == k), sum(y == k))
err = z/sum(h2.comparison == k)
err
k=2
x = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "g"]
y = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "b"]
z = min(sum(x == k), sum(y == k))
err = z/sum(h2.comparison == k)
err
kmRes = kmeans(pca.reduced, dist.euclidean, centers = k)
pca.reduced
kmRes = kmeans(pca.reduced, dist.euclidean, centers = k)
kmRes = kmeans(pca.reduced, centers = k)
kmRes
kmRes$cluster
pca.reduced = pca$x[,1:18]
error <- function(data, k){
errors = c(rep(0,k))
for(i in 1:k){
x = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "g"]
y = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "b"]
z = min(sum(x == k), sum(y == k))
errors[i] = z/sum(h2.comparison == k)
}
return(mean(errors))
}
errors = as.data.frame(matrix(0,nrow = 20, ncol = 5))
for(k in 2:5){
errorList = list()
for(j in 1:20){
print(j)
kmRes = kmeans(pca.reduced, centers = k)
newdata = cbind(kmRes$cluster, mydata$V35)
errors[j, k] = error(newdata,k)
}
}
errors
error <- function(data, k){
errorList = c(rep(0,k))
for(i in 1:k){
x = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "g"]
y = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "b"]
z = min(sum(x == k), sum(y == k))
err = z/sum(h2.comparison == k)
print(err)
errorList[i] = err
}
return(mean(errorList))
}
errors = as.data.frame(matrix(0,nrow = 20, ncol = 5))
for(k in 2:5){
errorList = list()
for(j in 1:20){
print(j)
kmRes = kmeans(pca.reduced, centers = k)
newdata = cbind(kmRes$cluster, mydata$V35)
errors[j, k] = error(newdata,k)
}
}
error <- function(data, k){
errorList = c(rep(0,k))
for(i in 1:k){
x = data[data[,1] == k & data[,2] == "g"]
y = data[data[,1] == k & data[,2] == "b"]
z = min(sum(x == k), sum(y == k))
err = z/sum(data == k)
print(err)
errorList[i] = err
}
return(mean(errorList))
}
errors = as.data.frame(matrix(0,nrow = 20, ncol = 5))
for(k in 2:5){
errorList = list()
for(j in 1:20){
print(j)
kmRes = kmeans(pca.reduced, centers = k)
newdata = cbind(kmRes$cluster, mydata$V35)
errors[j, k] = error(newdata,k)
}
}
print(data)
error <- function(data, k){
print(data)
errorList = c(rep(0,k))
for(i in 1:k){
x = data[data[,1] == k & data[,2] == "g"]
y = data[data[,1] == k & data[,2] == "b"]
z = min(sum(x == k), sum(y == k))
err = z/sum(data == k)
errorList[i] = err
}
return(mean(errorList))
}
errors = as.data.frame(matrix(0,nrow = 20, ncol = 5))
for(k in 2:5){
errorList = list()
for(j in 1:20){
print(j)
kmRes = kmeans(pca.reduced, centers = k)
newdata = cbind(kmRes$cluster, mydata$V35)
errors[j, k] = error(newdata,k)
}
}
error <- function(data, k){
errorList = c(rep(0,k))
for(i in 1:k){
x = data[data[,1] == i & data[,2] == "g"]
y = data[data[,1] == i & data[,2] == "b"]
z = min(sum(x == i), sum(y == i))
err = z/sum(data == i)
errorList[i] = err
}
return(mean(errorList))
}
errors = as.data.frame(matrix(0,nrow = 20, ncol = 5))
for(k in 2:5){
errorList = list()
for(j in 1:20){
print(j)
kmRes = kmeans(pca.reduced, centers = k)
newdata = cbind(kmRes$cluster, mydata$V35)
errors[j, k] = error(newdata,k)
}
}
errors
boxplot(errors)
boxplot(errors, main = "PCA Reduced K-means errors")
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
summary(college)
library(test2.R)
import(test2)
library(test2)
college <- scan(file.choose())
p = (m-u)/(s/sqrt(n))
#B)
#H0 = u <= 57,000     H1 = u > 57,000
#reject if H1 << 57,000
#alpha = .05
u=57000
n=340
m = mean(college)
l = length(college)
v = var(college)
s = sd(college)
p = (m-u)/(s/sqrt(n))
t = t.test(college, mu = 57000, alternative = 'l')
t
?t.test
values(t.test)
t$p.value
t = t.test(college, mu = 57000, alternative = 'l')
t = t.test(college, mu = 57000, alternative = 'g')
t
#95 percent confidence interval:
#  36.06240 48.19622
t.test(ozone, mu=50, alternative = 'l')
ozone <- scan(file.choose())
#95 percent confidence interval:
#  36.06240 48.19622
t.test(ozone, mu=50, alternative = 'l')
t.test(ozone)
#95 percent confidence interval:
#  36.06240 48.19622
t.test(ozone, mu=50, alternative = 'l')
t.test(ozone)
mean(ozone)
u = 50
n = length(ozone)
m = mean(ozone)
s = sd(ozone)
z = (m-u)/(s/sqrt(n))
z
1-pnorm(z)
pnorm(z)
qqnorm(college, main = "College qq-plot")
s = sd(college)
n=length(college)
#a =.05, so reject. U > 57,000
z=1.96
i1 = z*(s/sqrt(n))
i1
i1 = z*(s/sqrt(n))
mean + i1
m + i1
m - i1
mean(college)
m = mean(college)
m
i1
m + i1
m - i1
t.test(college)
i1 = z*(s/sqrt(n))
m + i1
m - i1
t = t.test(college)
values(t)
?t.test
t.conf.int
t = t.test(college)
t$conf.int
n = length(ozone)
m = mean(ozone)
s = sd(ozone)
z=1.96
i2 = z*(s/sqrt(n))
m + i2
m - i2
i2
#install.packages('mice')
#library('mice')
setwd('C:\\Users\\Joshua\\Documents\\GitHub\\B365-Final-Project')
library("caret")
options(scipen = 50)
data.train <- read.csv("train_p.csv", header = TRUE)
library("ggplot2")
options(scipen=100)
train <- read.csv("train.csv", header = TRUE)
t = train
claim = as.numeric(table(t$target)[2])
t[t<0] = -1
# Keep track of features with missing entries
missing = c()
par(mfrow=c(3,2))
# iterate through all features
for(f in colnames(train)){
# Find number of missing entries
missing_len = length(train[train[,f] <0,f])
# If there are missing entries find additional info
if(missing_len > 0){
# Append to missing vector
missing = append(missing, f)
# Calculate missing percentage vs all rows
missing_perc = missing_len / nrow(t) * 100
# Calculate the precentage of values in the feature that are both missing and have a target id of 1
target_perc = nrow(subset(t, t[f]<0 & t['target']==1))/length(t[t[,f] <0,f]) *100
target_perc2 = nrow(subset(t, t[f]<0 & t['target']==1))/claim *100
print(c(f, missing_len, missing_perc, target_perc2))
print(class(t[,f]))   # Prints the datatype of feature
# plot the histogram of entries and convert to probabilties instead of frequencies
h <- hist(t[,f], plot=FALSE)
h$counts=h$counts/sum(h$counts)
plot(h, main=f, xlab= "Value",ylab="Probability")
}
}
# Convert -1 to 2
for(feature in c("ps_car_03_cat","ps_car_05_cat")){
num = length(t[t[,feature] <0,feature])
t[t[,feature] <0,feature] = rep(2,num)
}
par(mfrow=c(1,3))
# This function creates a Discrete Random Variable DRV
# This assigns the a probabilty to each actual value, uses these probabilites to create a DRV and then replaces missing values from the DRV
randomVariable = function(feature){
values = c()              # Stores the discrete values
# Find distinct values using table
vals = as.data.frame(table(t[feature]))
probs = c()               # Stores the probabilities of each discrete value
base = nrow(t)            # Base number of values in feature
# Iterate through values
for(i in c(1:nrow(vals))){
val = as.numeric(as.character(vals[i,1]))
if(val <0){
base = base - vals[i,2]
} else {
values = append(values,val)
probs = append(probs,as.numeric(as.character(vals[i,2]))/base)
}
}
print(values)
new_vals = sample(x = values, size = vals[1,2], replace = T, prob = probs)
t[t[,feature] <0,feature] = new_vals
if(1){
print(ggplot(t, aes(x= t[, feature])) + geom_histogram(aes(y = (..count..)/sum(..count..)),binwidth=.5, colour="black", fill="white")+
labs(x = "Value", y = "Probabilty", title="Probability Distribution of Train before"))
print(ggplot() + aes(as.vector(new_vals)) + geom_histogram(aes(y = (..count..)/sum(..count..)),binwidth=.5, colour="black", fill="white")+
labs(x = "Value", y = "Probabilty", title="New Values"))
print(ggplot(t, aes(x= t[, feature])) + geom_histogram(aes(y = (..count..)/sum(..count..)),binwidth=.5, colour="black", fill="white")+
labs(x = "Value", y = "Probabilty", title="Train with replaced values"))
}
print(sum(t[,feature] < 0))
return(t)
}
# Replace values from a feature based DRV
for(feature in c("ps_ind_02_cat","ps_ind_04_cat","ps_ind_05_cat","ps_car_01_cat","ps_car_07_cat","ps_car_09_cat")){
t<-randomVariable(feature)
}
# Third Countinous numeric values, range beteen 0 and 1
# Feature            # of missing       % of entries missing     % missing claim vs total claim
# "ps_car_14"        "42620"            "7.16047391517644" "7.94228819028303"
# This single continous feature has a value between 0 and 1, The fact there's a low number of missing entries for this feature (7.2%) combined with a
# low % of missing that are a claim vs total claim, makes this a good candidate to replace missing with valus drawn from a Continous Random Variable
# model after the distribution of the feature itself. This feature has a lognormal distribution
contRandVar = function(feature){
actual = t[t[,feature]>=0,feature]
print(c(mean(actual),sd(actual)))
hist(actual, main="Actual")
log_actual = log10(actual)
hist(log_actual, main="Log10 Actual")
## Find expected value and standard deviation used in rlnorm with the log10 of the non-missing values
expected = mean(log_actual)
stand = sd(log_actual)
location = log(expected^2/sqrt(stand^2+expected^2))
shape = sqrt(log(1+(stand^2/expected^2)))
# Create a test sample to check if valid/close
test = rlnorm(n=50000,location-.1, shape)
# Visual inspection of the test to the actual
print(c(mean(test),sd(test), class(test)))
hist(actual, main="Actual",
prob = TRUE, # show densities instead of frequencies
xlab = "Value")
lines(density(test), # density plot
lwd = 2, # thickness of line
col = "chocolate3")
# Replace missing entries
num = length(t[t[,feature]<0,feature])
print(num)
hist(t[,feature])
new_vals = rlnorm(n=num,location-.1, shape)
t[t[,feature]<0,feature] = new_vals
hist(t[,feature])
# plot graphs for visual check
hist(actual, main="Missing Replaced",
prob = TRUE, # show densities instead of frequencies
xlab = "Value")
lines(density(new_vals), # density plot
lwd = 2, # thickness of line
col = "chocolate3")
return(t)
}
t<-contRandVar("ps_car_14")
t<-contRandVar("ps_reg_03")
#Credit - Ken Williams https://stackoverflow.com/questions/2547402/is-there-a-built-in-function-for-finding-the-mode?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa
Mode <- function(x) {
ux <- unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
mode_repl = function(feature){
# Non-missing values
actual = t[t[,feature]>=0,feature]
# replace value is set to mode
replace = Mode(actual)
# number of missing
num = length(t[t[,feature]<0,feature])
# Repalce values
t[t[,feature]<0,feature] = rep(replace*num)
return(t)
}
# Replace the missing values with mode
for(feature in c("ps_car_11","ps_car_12","ps_car_02_cat")){
t<-mode_repl(feature)
}
summary(t)
write.table(t, "train_p.csv", sep= ",")
library("ggplot2")
options(scipen=100)
test <- read.csv("test.csv", header = TRUE)
str(test)
ncol(test)
t = test
#claim = as.numeric(table(t$target)[2])
t[t<0] = -1
# Keep track of features with missing entries
missing = c()
par(mfrow=c(3,2))
# Convert -1 to 2
for(feature in c("ps_car_03_cat","ps_car_05_cat")){
num = length(t[t[,feature] <0,feature])
t[t[,feature] <0,feature] = rep(2,num)
}
par(mfrow=c(1,3))
# This function creates a Discrete Random Variable DRV
# This assigns the a probabilty to each actual value, uses these probabilites to create a DRV and then replaces missing values from the DRV
randomVariable = function(feature){
values = c()              # Stores the discrete values
# Find distinct values using table
vals = as.data.frame(table(t[feature]))
probs = c()               # Stores the probabilities of each discrete value
base = nrow(t)            # Base number of values in feature
# Iterate through values
for(i in c(1:nrow(vals))){
val = as.numeric(as.character(vals[i,1]))
if(val <0){
base = base - vals[i,2]
} else {
values = append(values,val)
probs = append(probs,as.numeric(as.character(vals[i,2]))/base)
}
}
print(values)
new_vals = sample(x = values, size = vals[1,2], replace = T, prob = probs)
t[t[,feature] <0,feature] = new_vals
if(1){
print(ggplot(t, aes(x= t[, feature])) + geom_histogram(aes(y = (..count..)/sum(..count..)),binwidth=.5, colour="black", fill="white")+
labs(x = "Value", y = "Probabilty", title="Probability Distribution of Train before"))
print(ggplot() + aes(as.vector(new_vals)) + geom_histogram(aes(y = (..count..)/sum(..count..)),binwidth=.5, colour="black", fill="white")+
labs(x = "Value", y = "Probabilty", title="New Values"))
print(ggplot(t, aes(x= t[, feature])) + geom_histogram(aes(y = (..count..)/sum(..count..)),binwidth=.5, colour="black", fill="white")+
labs(x = "Value", y = "Probabilty", title="Train with replaced values"))
}
print(sum(t[,feature] < 0))
return(t)
}
# Replace values from a feature based DRV
for(feature in c("ps_ind_02_cat","ps_ind_04_cat","ps_ind_05_cat","ps_car_01_cat","ps_car_07_cat","ps_car_09_cat")){
t<-randomVariable(feature)
}
# Third Countinous numeric values, range beteen 0 and 1
# Feature            # of missing       % of entries missing     % missing claim vs total claim
# "ps_car_14"        "42620"            "7.16047391517644" "7.94228819028303"
# This single continous feature has a value between 0 and 1, The fact there's a low number of missing entries for this feature (7.2%) combined with a
# low % of missing that are a claim vs total claim, makes this a good candidate to replace missing with valus drawn from a Continous Random Variable
# model after the distribution of the feature itself. This feature has a lognormal distribution
contRandVar = function(feature){
actual = t[t[,feature]>=0,feature]
print(c(mean(actual),sd(actual)))
hist(actual, main="Actual")
log_actual = log10(actual)
hist(log_actual, main="Log10 Actual")
## Find expected value and standard deviation used in rlnorm with the log10 of the non-missing values
expected = mean(log_actual)
stand = sd(log_actual)
location = log(expected^2/sqrt(stand^2+expected^2))
shape = sqrt(log(1+(stand^2/expected^2)))
# Create a test sample to check if valid/close
test = rlnorm(n=50000,location-.1, shape)
# Visual inspection of the test to the actual
print(c(mean(test),sd(test), class(test)))
hist(actual, main="Actual",
prob = TRUE, # show densities instead of frequencies
xlab = "Value")
lines(density(test), # density plot
lwd = 2, # thickness of line
col = "chocolate3")
# Replace missing entries
num = length(t[t[,feature]<0,feature])
print(num)
hist(t[,feature])
new_vals = rlnorm(n=num,location-.1, shape)
t[t[,feature]<0,feature] = new_vals
hist(t[,feature])
# plot graphs for visual check
hist(actual, main="Missing Replaced",
prob = TRUE, # show densities instead of frequencies
xlab = "Value")
lines(density(new_vals), # density plot
lwd = 2, # thickness of line
col = "chocolate3")
return(t)
}
t<-contRandVar("ps_car_14")
t<-contRandVar("ps_reg_03")
#Credit - Ken Williams https://stackoverflow.com/questions/2547402/is-there-a-built-in-function-for-finding-the-mode?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa
Mode <- function(x) {
ux <- unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
mode_repl = function(feature){
# Non-missing values
actual = t[t[,feature]>=0,feature]
# replace value is set to mode
replace = Mode(actual)
# number of missing
num = length(t[t[,feature]<0,feature])
# Repalce values
t[t[,feature]<0,feature] = rep(replace*num)
return(t)
}
# Replace the missing values with mode
for(feature in c("ps_car_11","ps_car_12","ps_car_02_cat")){
t<-mode_repl(feature)
}
summary(t)
write.table(t, "test_p.csv", sep= ",")
data.train <- read.csv("train_p.csv", header = TRUE)
data.test <- read.csv("test_p.csv", header = TRUE)
test_ids = data.test$id
data.test <- subset(data.test,select = -c(id))
data.train <- subset(data.train,select = -c(id))
training_classes = data.train$target
data.train <- subset(data.train,select = -c(target))
training_size = nrow(data.train)
total_data = rbind(data.train, data.test)
#install.packages("xgboost")
library('xgboost')
bstDense <- xgboost(data = as.matrix(training), label = training_classes, missing = NA, max_depth = 5, eta = 1, nthread = 2, nrounds = 10, objective = "binary:logistic")
training = as.data.frame(new_data[1:training_size,])
test = new_data[(training_size+1):nrow(new_data),]
bstDense <- xgboost(data = as.matrix(data.train), label = training_classes, missing = NA, max_depth = 5, eta = 1, nthread = 2, nrounds = 10, objective = "binary:logistic")
bstDense <- xgboost(data = as.matrix(data.train), label = training_classes, missing = NA, max_depth = 8, eta = 1, nthread = 2, nrounds = 5, objective = "binary:logistic")
?xgboost
bstDense <- xgboost(data = as.matrix(data.train), label = training_classes, missing = NA, max_depth = 4, nthread = 2, nrounds = 10, objective = "binary:logistic")
bstDense <- xgboost(data = as.matrix(data.train), eta = .1, label = training_classes, missing = NA, max_depth = 4, nthread = 2, nrounds = 10, objective = "binary:logistic")
bstDense <- xgboost(data = as.matrix(data.train), eta = .1, label = training_classes, missing = NA, max_depth = 10, nthread = 2, nrounds = 10, objective = "binary:logistic")
bstDense <- xgboost(data = as.matrix(data.train), eta = .1, label = training_classes, max_depth = 14, nthread = 2, nrounds = 10, objective = "binary:logistic")
bstDense <- xgboost(data = as.matrix(data.train), subsample =.5,eta = .05, label = training_classes, max_depth = 14, nthread = 2, nrounds = 10, objective = "binary:logistic")
