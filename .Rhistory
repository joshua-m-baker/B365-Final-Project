sum(x == 1) #8
8/20
x = h.comparison[h.comparison[,1] == 2 & h.comparison[,2] == "g"]
sum(x == 2) #3
x = h.comparison[h.comparison[,1] == 2 & h.comparison[,2] == "b"]
sum(x == 2) #g = 27
3/30
8/20
h2 = hclust(dist(d.reduced, method = "euclidean"), method = "complete")
h.comparison
plot(h)
plot(h, main = "2.2.1")
sum(h.comparison == 1) #Cluster 1 size = 20
sum(h.comparison == 2) #Cluster 2 size = 30
sum(h.comparison == "g") #39
sum(h.comparison == "b") #11
x = h.comparison[h.comparison[,1] == 1 & h.comparison[,2] == "g"]
sum(x == 1) #b = 8, g = 12
d.pca = prcomp(d2.data[,-35])
summary(d.pca)
summary(d.pca)
d.reduced = d.pca$x[,1:8]
h2 = hclust(dist(d.reduced, method = "euclidean"), method = "complete")
plot(h2)
plot(h2, main = "2.2.3")
h2.cut = cutree(h2, k=2)
h2.comparison = cbind(h2.cut, d2.data$V35)
h2.comparison
sum(h2.comparison == 1) #Cluster 1 size = 39
sum(h2.comparison == 2) #Cluster 2 size = 11
sum(h.comparison == "g") #39
sum(h.comparison == "b") #11
sum(h2.comparison == "g") #39
sum(h2.comparison == "b") #11
x = h2.comparison[h2.comparison[,1] == 1 & h2.comparison[,2] == "b"]
sum(x == 1) #8
x = h2.comparison[h2.comparison[,1] == 2 & h2.comparison[,2] == "g"]
sum(x == 2) #6
x = h2.comparison[h2.comparison[,1] == 2 & h2.comparison[,2] == "b"]
sum(x == 2) #6
5/36
sum(h2.comparison == 1) #Cluster 1 size = 14
h2.comparison == 1
sum(h2.comparison == 1) #Cluster 1 size = 14
sum(h2.comparison == 2) #Cluster 2 size = 36
x = h2.comparison[h2.comparison[,1] == 1 & h2.comparison[,2] == "b"]
x = h2.comparison[h2.comparison[,1] == 1 & h2.comparison[,2] == "g"]
sum(x == 1) #6
6/14
.429+.139
.568/2
d = fread("https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data")
d.data = d[,-35]
pca = prcomp(d.data)
summary(pca)
plot(pca, type = "l")
plot(pca, type = "l", main = "PC Variance")
plot(h, main = "2.2.1")
plot(h2, main = "2.2.3")
library(data.table)
d = fread("https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data")
d.data = d[,-35]
pca = prcomp(d.data)
summary(pca)
plot(pca, type = "l", main = "PC Variance")
screeplot(pca)
#newdata
pca$x
summary(pca)
pca.reduced = pca$x[,1:18]
pca.reduced
min(1,2)
x = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "g"]
y = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "b"]
x
y
h2.comparison
z = min(sum(x == k), sum(y == k))
err = z/sum(h2.comparison == k)
er
err
z
x = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "g"]
x
h2.comparison
x = h2.comparison[h2.comparison[,1] == 1 & h2.comparison[,2] == "g"]
sum(x == 1) #6
k=1
x = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "g"]
y = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "b"]
z = min(sum(x == k), sum(y == k))
err = z/sum(h2.comparison == k)
err
k=2
x = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "g"]
y = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "b"]
z = min(sum(x == k), sum(y == k))
err = z/sum(h2.comparison == k)
err
kmRes = kmeans(pca.reduced, dist.euclidean, centers = k)
pca.reduced
kmRes = kmeans(pca.reduced, dist.euclidean, centers = k)
kmRes = kmeans(pca.reduced, centers = k)
kmRes
kmRes$cluster
pca.reduced = pca$x[,1:18]
error <- function(data, k){
errors = c(rep(0,k))
for(i in 1:k){
x = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "g"]
y = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "b"]
z = min(sum(x == k), sum(y == k))
errors[i] = z/sum(h2.comparison == k)
}
return(mean(errors))
}
errors = as.data.frame(matrix(0,nrow = 20, ncol = 5))
for(k in 2:5){
errorList = list()
for(j in 1:20){
print(j)
kmRes = kmeans(pca.reduced, centers = k)
newdata = cbind(kmRes$cluster, mydata$V35)
errors[j, k] = error(newdata,k)
}
}
errors
error <- function(data, k){
errorList = c(rep(0,k))
for(i in 1:k){
x = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "g"]
y = h2.comparison[h2.comparison[,1] == k & h2.comparison[,2] == "b"]
z = min(sum(x == k), sum(y == k))
err = z/sum(h2.comparison == k)
print(err)
errorList[i] = err
}
return(mean(errorList))
}
errors = as.data.frame(matrix(0,nrow = 20, ncol = 5))
for(k in 2:5){
errorList = list()
for(j in 1:20){
print(j)
kmRes = kmeans(pca.reduced, centers = k)
newdata = cbind(kmRes$cluster, mydata$V35)
errors[j, k] = error(newdata,k)
}
}
error <- function(data, k){
errorList = c(rep(0,k))
for(i in 1:k){
x = data[data[,1] == k & data[,2] == "g"]
y = data[data[,1] == k & data[,2] == "b"]
z = min(sum(x == k), sum(y == k))
err = z/sum(data == k)
print(err)
errorList[i] = err
}
return(mean(errorList))
}
errors = as.data.frame(matrix(0,nrow = 20, ncol = 5))
for(k in 2:5){
errorList = list()
for(j in 1:20){
print(j)
kmRes = kmeans(pca.reduced, centers = k)
newdata = cbind(kmRes$cluster, mydata$V35)
errors[j, k] = error(newdata,k)
}
}
print(data)
error <- function(data, k){
print(data)
errorList = c(rep(0,k))
for(i in 1:k){
x = data[data[,1] == k & data[,2] == "g"]
y = data[data[,1] == k & data[,2] == "b"]
z = min(sum(x == k), sum(y == k))
err = z/sum(data == k)
errorList[i] = err
}
return(mean(errorList))
}
errors = as.data.frame(matrix(0,nrow = 20, ncol = 5))
for(k in 2:5){
errorList = list()
for(j in 1:20){
print(j)
kmRes = kmeans(pca.reduced, centers = k)
newdata = cbind(kmRes$cluster, mydata$V35)
errors[j, k] = error(newdata,k)
}
}
error <- function(data, k){
errorList = c(rep(0,k))
for(i in 1:k){
x = data[data[,1] == i & data[,2] == "g"]
y = data[data[,1] == i & data[,2] == "b"]
z = min(sum(x == i), sum(y == i))
err = z/sum(data == i)
errorList[i] = err
}
return(mean(errorList))
}
errors = as.data.frame(matrix(0,nrow = 20, ncol = 5))
for(k in 2:5){
errorList = list()
for(j in 1:20){
print(j)
kmRes = kmeans(pca.reduced, centers = k)
newdata = cbind(kmRes$cluster, mydata$V35)
errors[j, k] = error(newdata,k)
}
}
errors
boxplot(errors)
boxplot(errors, main = "PCA Reduced K-means errors")
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
summary(college)
library(test2.R)
import(test2)
library(test2)
college <- scan(file.choose())
p = (m-u)/(s/sqrt(n))
#B)
#H0 = u <= 57,000     H1 = u > 57,000
#reject if H1 << 57,000
#alpha = .05
u=57000
n=340
m = mean(college)
l = length(college)
v = var(college)
s = sd(college)
p = (m-u)/(s/sqrt(n))
t = t.test(college, mu = 57000, alternative = 'l')
t
?t.test
values(t.test)
t$p.value
t = t.test(college, mu = 57000, alternative = 'l')
t = t.test(college, mu = 57000, alternative = 'g')
t
#95 percent confidence interval:
#  36.06240 48.19622
t.test(ozone, mu=50, alternative = 'l')
ozone <- scan(file.choose())
#95 percent confidence interval:
#  36.06240 48.19622
t.test(ozone, mu=50, alternative = 'l')
t.test(ozone)
#95 percent confidence interval:
#  36.06240 48.19622
t.test(ozone, mu=50, alternative = 'l')
t.test(ozone)
mean(ozone)
u = 50
n = length(ozone)
m = mean(ozone)
s = sd(ozone)
z = (m-u)/(s/sqrt(n))
z
1-pnorm(z)
pnorm(z)
qqnorm(college, main = "College qq-plot")
s = sd(college)
n=length(college)
#a =.05, so reject. U > 57,000
z=1.96
i1 = z*(s/sqrt(n))
i1
i1 = z*(s/sqrt(n))
mean + i1
m + i1
m - i1
mean(college)
m = mean(college)
m
i1
m + i1
m - i1
t.test(college)
i1 = z*(s/sqrt(n))
m + i1
m - i1
t = t.test(college)
values(t)
?t.test
t.conf.int
t = t.test(college)
t$conf.int
n = length(ozone)
m = mean(ozone)
s = sd(ozone)
z=1.96
i2 = z*(s/sqrt(n))
m + i2
m - i2
i2
#install.packages('mice')
#library('mice')
setwd('C:\\Users\\Joshua\\Documents\\GitHub\\B365-Final-Project')
#install.packages('mice')
#library('mice')
#setwd('C:\\Users\\Joshua\\Documents\\GitHub\\B365-Final-Project')
library("caret")
data.train <- read.csv("train.csv", header = TRUE,na.strings = "-1" )
data.test <- read.csv("test.csv", header = TRUE,na.strings = "-1")
data.train[data.train<0] = NA
data.test[data.test<0] = NA
data.test = cbind(data.test,NA_count = rowSums(is.na(data.test)))
data.train = cbind(data.train,NA_count = rowSums(is.na(data.train)))
plot(data.train$NA_count, data.train$target)
test_ids = data.test$id
data.test <- subset(data.test,select = -c(id))
data.train <- subset(data.train,select = -c(id))
training_classes = data.train$target
data.train <- subset(data.train,select = -c(target))
training_size = nrow(data.train)
data.test <- subset(data.test,select = -c(ps_car_03_cat,ps_car_05_cat,ps_reg_03))
data.train <- subset(data.train,select = -c(ps_car_03_cat,ps_car_05_cat,ps_reg_03))
total_data = rbind(data.train, data.test)
# total_data <- amelia(total_data, idvars = c("ps_ind_14", "NA_count", "ps_ind_09_bin"), m=3, p2s = 1)
# summary(total_data)
col_means = colMeans(total_data, na.rm = TRUE)
for(i in 1:ncol(total_data)){
total_data[is.na(total_data[,i]), i] <- col_means[i]
}
#summary(total_data)
# PCA Stuff
pca <- prcomp(total_data, scale. = TRUE)
new_data = pca$x[,1:16]
training = as.data.frame(new_data[1:training_size,])
test = new_data[(training_size+1):nrow(new_data),]
d2 = cbind(training, training_classes)
m = train(training,as.factor(training_classes),'nb',trControl=trainControl(method='cv',number=10))
fit <- lm(training_classes~., data=training)
fit
p = predict(fit, test)
typeof(test)
p = predict(fit, as.data.frame(test))
p
new_data
#submission = cbind(id = test_ids, target = pred[,2])
submission = cbind(id = test_ids, target = p)
write.table(submission, file="submission.csv", row.names = FALSE, col.names = TRUE, sep=",")
head(pred)
p1 = predict(m$finalModel,test)
]
p
p2 = abs(p)
#submission = cbind(id = test_ids, target = pred[,2])
submission = cbind(id = test_ids, target = p)
write.table(submission, file="submission.csv", row.names = FALSE, col.names = TRUE, sep=",")
summary(p2)
install.packages("xgboost")
library('xgboost')
bstDense <- xgboost(data = training, label = training_classes, max_depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")
bstDense <- xgboost(data = as.matrix(training), label = training_classes, max_depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")
prediction = predict(bstDense, test)
prediction
#submission = cbind(id = test_ids, target = pred[,2])
submission = cbind(id = test_ids, target = p)
write.table(submission, file="submission.csv", row.names = FALSE, col.names = TRUE, sep=",")
#submission = cbind(id = test_ids, target = pred[,2])
submission = cbind(id = test_ids, target = prediction)
write.table(submission, file="submission.csv", row.names = FALSE, col.names = TRUE, sep=",")
options(scipen = 50)
#submission = cbind(id = test_ids, target = pred[,2])
submission = cbind(id = test_ids, target = prediction)
write.table(submission, file="submission.csv", row.names = FALSE, col.names = TRUE, sep=",")
summary(prediction)
summary(submission)
tail(submission)
#submission = cbind(id = test_ids, target = pred[,2])
submission = cbind(id = test_ids, target = prediction)
write.table(submission, file="submission.csv", row.names = FALSE, col.names = TRUE, sep=",")
data.train <- read.csv("train.csv", header = TRUE,na.strings = "-1" )
data.test <- read.csv("test.csv", header = TRUE,na.strings = "-1")
data.train[data.train<0] = NA
data.test[data.test<0] = NA
data.test = cbind(data.test,NA_count = rowSums(is.na(data.test)))
data.train = cbind(data.train,NA_count = rowSums(is.na(data.train)))
test_ids = data.test$id
training_classes = data.train$target
data.train <- subset(data.train,select = -c(target))
training_size = nrow(data.train)
test_ids = data.test$id
data.test <- subset(data.test,select = -c(id))
data.train <- subset(data.train,select = -c(id))
training_classes = data.train$target
data.train <- subset(data.train,select = -c(target))
training_size = nrow(data.train)
training_classes = data.train$target
data.train <- subset(data.train,select = -c(target))
head(data.train)
training_classes
data.train <- read.csv("train.csv", header = TRUE,na.strings = "-1" )
data.test <- read.csv("test.csv", header = TRUE,na.strings = "-1")
data.train[data.train<0] = NA
data.test[data.test<0] = NA
data.test = cbind(data.test,NA_count = rowSums(is.na(data.test)))
data.train = cbind(data.train,NA_count = rowSums(is.na(data.train)))
test_ids = data.test$id
test_ids = data.test$id
head(test_ids)
data.test <- subset(data.test,select = -c(id))
data.train <- subset(data.train,select = -c(id))
training_classes = data.train$target
data.train <- subset(data.train,select = -c(target))
training_size = nrow(data.train)
total_data = rbind(data.train, data.test)
bstDense <- xgboost(data = as.matrix(training), label = training_classes, max_depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")
prediction = predict(bstDense, test)
prediction
bstDense <- xgboost(data = as.matrix(training), label = training_classes, missing = NA, max_depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")
prediction = predict(bstDense, test)
prediction
bstDense <- xgboost(data = as.matrix(training), label = training_classes, missing = NA, max_depth = 4, eta = 1, nthread = 2, nrounds = 5, objective = "binary:logistic")
bstDense <- xgboost(data = as.matrix(training), label = training_classes, missing = NA, max_depth = 8, eta = 1, nthread = 2, nrounds = 5, objective = "binary:logistic")
bstDense <- xgboost(data = as.matrix(training), label = training_classes, missing = NA, max_depth = 20, eta = 1, nthread = 2, nrounds = 3, objective = "binary:logistic")
prediction = predict(bstDense, test)
prediction
#submission = cbind(id = test_ids, target = pred[,2])
submission = cbind(id = test_ids, target = prediction)
write.table(submission, file="submission.csv", row.names = FALSE, col.names = TRUE, sep=",")
test_ids = data.test$id
data.test <- subset(data.test,select = -c(id))
data.train <- subset(data.train,select = -c(id))
training_classes = data.train$target
data.train <- subset(data.train,select = -c(target))
training_size = nrow(data.train)
data.test <- subset(data.test,select = -c(ps_car_03_cat,ps_car_05_cat,ps_reg_03))
data.train <- subset(data.train,select = -c(ps_car_03_cat,ps_car_05_cat,ps_reg_03))
total_data = rbind(data.train, data.test)
# total_data <- amelia(total_data, idvars = c("ps_ind_14", "NA_count", "ps_ind_09_bin"), m=3, p2s = 1)
# summary(total_data)
col_means = colMeans(total_data, na.rm = TRUE)
for(i in 1:ncol(total_data)){
total_data[is.na(total_data[,i]), i] <- col_means[i]
}
#summary(total_data)
# PCA Stuff
pca <- prcomp(total_data, scale. = TRUE)
summary(pca)
#install.packages('mice')
#library('mice')
#setwd('C:\\Users\\Joshua\\Documents\\GitHub\\B365-Final-Project')
library("caret")
options(scipen = 50)
data.train <- read.csv("train.csv", header = TRUE,na.strings = "-1" )
data.test <- read.csv("test.csv", header = TRUE,na.strings = "-1")
data.train[data.train<0] = NA
data.test[data.test<0] = NA
data.test = cbind(data.test,NA_count = rowSums(is.na(data.test)))
data.train = cbind(data.train,NA_count = rowSums(is.na(data.train)))
test_ids = data.test$id
data.test <- subset(data.test,select = -c(id))
data.train <- subset(data.train,select = -c(id))
training_classes = data.train$target
data.train <- subset(data.train,select = -c(target))
training_size = nrow(data.train)
data.test <- subset(data.test,select = -c(ps_car_03_cat,ps_car_05_cat,ps_reg_03))
data.train <- subset(data.train,select = -c(ps_car_03_cat,ps_car_05_cat,ps_reg_03))
total_data = rbind(data.train, data.test)
# total_data <- amelia(total_data, idvars = c("ps_ind_14", "NA_count", "ps_ind_09_bin"), m=3, p2s = 1)
# summary(total_data)
col_means = colMeans(total_data, na.rm = TRUE)
for(i in 1:ncol(total_data)){
total_data[is.na(total_data[,i]), i] <- col_means[i]
}
#summary(total_data)
# PCA Stuff
pca <- prcomp(total_data, scale. = TRUE)
summary(pca)
#summary(total_data)
# PCA Stuff
pca <- prcomp(total_data)
new_data = pca$x[,1:16]
training = as.data.frame(new_data[1:training_size,])
test = new_data[(training_size+1):nrow(new_data),]
library('xgboost')
bstDense <- xgboost(data = as.matrix(training), label = training_classes, missing = NA, max_depth = 20, eta = 1, nthread = 2, nrounds = 3, objective = "binary:logistic")
prediction = predict(bstDense, test)
prediction
#submission = cbind(id = test_ids, target = pred[,2])
submission = cbind(id = test_ids, target = prediction)
write.table(submission, file="submission.csv", row.names = FALSE, col.names = TRUE, sep=",")
#bstDense <- xgboost(data = as.matrix(training), label = training_classes, missing = NA, max_depth = 20, eta = 1, nthread = 2, nrounds = 3, objective = "binary:logistic")
bstDense <- xgboost(data = as.matrix(data.train), label = training_classes, missing = NA, max_depth = 20, eta = 1, nthread = 2, nrounds = 3, objective = "binary:logistic")
#bstDense <- xgboost(data = as.matrix(training), label = training_classes, missing = NA, max_depth = 20, eta = 1, nthread = 2, nrounds = 3, objective = "binary:logistic")
bstDense <- xgboost(data = as.matrix(data.train), label = training_classes, missing = NA, max_depth = 4, eta = 1, nthread = 2, nrounds = 10, objective = "binary:logistic")
#bstDense <- xgboost(data = as.matrix(training), label = training_classes, missing = NA, max_depth = 20, eta = 1, nthread = 2, nrounds = 3, objective = "binary:logistic")
bstDense <- xgboost(data = as.matrix(data.train), label = training_classes, missing = NA, max_depth = 20, eta = 1, nthread = 2, nrounds = 10, objective = "binary:logistic")
#summary(total_data)
# PCA Stuff
pca <- prcomp(total_data, scale.=TRUE)
#new_data = pca$x[,1:16]
new_data=pca$x
training = as.data.frame(new_data[1:training_size,])
test = new_data[(training_size+1):nrow(new_data),]
bstDense <- xgboost(data = as.matrix(training), label = training_classes, missing = NA, max_depth = 20, eta = 1, nthread = 2, nrounds = 3, objective = "binary:logistic")
bstDense <- xgboost(data = as.matrix(training), label = training_classes, missing = NA, max_depth = 4, eta = 1, nthread = 2, nrounds = 3, objective = "binary:logistic")
bstDense <- xgboost(data = as.matrix(training), label = training_classes, missing = NA, max_depth = 4, eta = 1, nthread = 2, nrounds = 10, objective = "binary:logistic")
bstDense <- xgboost(data = as.matrix(training), label = training_classes, missing = NA, max_depth = 10, eta = 1, nthread = 2, nrounds = 5, objective = "binary:logistic")
head(training)
prediction = predict(bstDense, test)
prediction
prediction = predict(bstDense, training)
prediction = predict(bstDense, test)
